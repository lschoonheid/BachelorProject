{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run as DNN\n",
    "import predict\n",
    "from dirs import *\n",
    "import efficiency\n",
    "from data_exploration.helpers import find_file, save\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import timeit\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_EXPORT = False\n",
    "\n",
    "event_name: str = \"event000001001\"\n",
    "event = DNN.get_featured_event(event_name)\n",
    "preload = True\n",
    "PATH_THR = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODELS_ROOT)\n",
    "print(SOLUTION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_model_h = DNN.get_model(preload=preload, save=DO_EXPORT, dir=SOLUTION_DIR, inname=\"my_model_h.h5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = event.features\n",
    "used_model: keras.models.Model = outrunner_model_h # type: ignore\n",
    "pred_matrix_limit: int = 10\n",
    "hit_id_test = 1\n",
    "hit_index_test = hit_id_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = event.hits[event.hits.hit_id == hit_id_test]\n",
    "z_pos = event.hits['z'][hit_index_test] > 0\n",
    "\n",
    "z_pos\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if z_pos:\n",
    "    cand_idx = np.where(event.hits['z'] > 0)[0]\n",
    "else:\n",
    "    cand_idx = np.where(event.hits['z'] < 0)[0]\n",
    "# TODO when z == 0\n",
    "\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_features = used_features[cand_idx]\n",
    "cand_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cand_idx(hit_id, hits):\n",
    "    hit_index = hit_id - 1\n",
    "    z_pos = hits['z'][hit_index] > 0\n",
    "    if z_pos:\n",
    "        cand_idx = np.where(event.hits['z'] > 0)[0]\n",
    "    else:\n",
    "        cand_idx = np.where(event.hits['z'] < 0)[0]    \n",
    "    return cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"eliminated\", len(get_cand_idx(hit_id_test, event.hits))/len(used_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_fast(model, features, hits: pd.DataFrame, hit_id: int, thr=0.85, batch_size: int | None = None, selector=get_cand_idx) -> npt.NDArray:\n",
    "    cand_idx = selector(hit_id, hits)\n",
    "    # cand_idx = np.arange(len(features))\n",
    "    cand_features = features[cand_idx]\n",
    "\n",
    "    Tx = np.zeros((len(cand_features), 10))\n",
    "    # Set first five columns of Tx to be the features of the hit with hit_id\n",
    "    # Shift hit_id -> hit_id - 1 because hit_id starts at 1 and index starts at 0\n",
    "    hit_index = hit_id - 1\n",
    "    Tx[:, :5] = np.tile(features[hit_index], (len(Tx), 1))\n",
    "    # Set last five columns of Tx to be the features of all hits\n",
    "    Tx[:, 5:] = cand_features\n",
    "\n",
    "    # Make prediction\n",
    "    batch_size = batch_size or round(len(Tx) / (5 * len(cand_idx)/len(features)))\n",
    "    pred_small = model.predict(Tx, batch_size=batch_size)[:, 0]  # type: ignore\n",
    "    pred = np.zeros(len(hits))\n",
    "    pred[cand_idx] = pred_small\n",
    "    \n",
    "     # TTA (test time augmentation)\n",
    "    \"\"\" TTA takes a similar concept but applies it during the testing or inference phase. Instead of making predictions on the original test samples alone, TTA generates multiple augmented versions of the test samples by applying various transformations or augmentations. The model then makes predictions on each augmented version, and the final prediction is obtained by aggregating the predictions from all the augmented samples. Common aggregation techniques include taking the average or the maximum probability across the augmented predictions. \"\"\"\n",
    "\n",
    "    # Take indices of prediction that have a prediction above the threshold\n",
    "    idx = np.where(pred > thr)[0]\n",
    "\n",
    "    # Filter Tx on predictions above threshold and swap first and last five columns\n",
    "    # TTA\n",
    "    Tx2 = np.zeros((len(idx), 10))\n",
    "    # print(len(cand_idx), len(Tx), len(Tx2))\n",
    "    Tx2[:, 5:] = Tx[idx, :5]\n",
    "    Tx2[:, :5] = Tx[idx, 5:]\n",
    "\n",
    "    # Predict again with swapped columns\n",
    "    pred1 = model.predict(Tx2, batch_size=batch_size)[:, 0]  # type: ignore\n",
    "\n",
    "    # Take average of predictions and swapped predictions\n",
    "    pred[idx] = (pred[idx] + pred1) / 2\n",
    "\n",
    "    return pred\n",
    "\n",
    "make_predict_fast(used_model, used_features, event.hits, hit_id_test)\n",
    "# timeit.timeit(lambda: make_predict_fast(used_model, used_features, event.hits, hit_id_test), number=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(\n",
    "    model, features: npt.NDArray, hits: pd.DataFrame, hit_id: int, thr=0.85, batch_size: int | None = None\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"Predict probability of each pair of hits with the last hit in the path. Generates a prediction array of length len(truth) with the probability of each hit belonging to the same track as hit_id.\"\"\"\n",
    "    Tx = np.zeros((len(hits), 10))\n",
    "    # Set first five columns of Tx to be the features of the hit with hit_id\n",
    "    # Shift hit_id -> hit_id - 1 because hit_id starts at 1 and index starts at 0\n",
    "    hit_index = hit_id - 1\n",
    "    Tx[:, :5] = np.tile(features[hit_index], (len(Tx), 1))\n",
    "    # Set last five columns of Tx to be the features of all hits\n",
    "    Tx[:, 5:] = features\n",
    "\n",
    "    # Make prediction\n",
    "    batch_size = batch_size or round(len(Tx) / 5)\n",
    "    pred = model.predict(Tx, batch_size=batch_size)[:, 0]  # type: ignore\n",
    "\n",
    "    # TTA (test time augmentation)\n",
    "    \"\"\" TTA takes a similar concept but applies it during the testing or inference phase. Instead of making predictions on the original test samples alone, TTA generates multiple augmented versions of the test samples by applying various transformations or augmentations. The model then makes predictions on each augmented version, and the final prediction is obtained by aggregating the predictions from all the augmented samples. Common aggregation techniques include taking the average or the maximum probability across the augmented predictions. \"\"\"\n",
    "\n",
    "    # Take indices of prediction that have a prediction above the threshold\n",
    "    idx = np.where(pred > thr)[0]\n",
    "\n",
    "    # Filter Tx on predictions above threshold and swap first and last five columns\n",
    "    # TTA\n",
    "    Tx2 = np.zeros((len(idx), 10))\n",
    "    Tx2[:, 5:] = Tx[idx, :5]\n",
    "    Tx2[:, :5] = Tx[idx, 5:]\n",
    "\n",
    "    # Predict again with swapped columns\n",
    "    pred1 = model.predict(Tx2, batch_size=batch_size)[:, 0]  # type: ignore\n",
    "\n",
    "    # Take average of predictions and swapped predictions\n",
    "    pred[idx] = (pred[idx] + pred1) / 2\n",
    "\n",
    "    return pred\n",
    "# timeit.timeit(lambda: make_predict(used_model, used_features, event.hits, hit_id_test, thr=PATH_THR, batch_size=None), number=100)\n",
    "make_predict(used_model, used_features, event.hits, hit_id_test, thr=PATH_THR, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true = make_predict(used_model, used_features, event.hits, hit_id_test, thr=PATH_THR, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = make_predict_fast(used_model, used_features, event.hits, hit_id_test, thr=PATH_THR, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true, p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_seeds(hits, thr: float = 300):\n",
    "    inner_idx = np.where(efficiency.add_r(hits, mode=\"hits\")['r'] < thr)[0]\n",
    "    return inner_idx\n",
    "select_seeds(event.hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = used_model.predict([[*used_features[0], *used_features[1]]])[:, 0]  # type: ignore\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrootenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
