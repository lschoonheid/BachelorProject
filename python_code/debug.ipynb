{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get things ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from data_exploration.helpers import find_file, save\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import trackml_copy as outrunner_code\n",
    "import trackml_2_solution_example as my_code\n",
    "\n",
    "DO_EXPORT = True\n",
    "DIRECTORY = my_code.DIRECTORY\n",
    "SOLUTION_DIR = my_code.SOLUTION_DIR\n",
    "\n",
    "\n",
    "DATA_ROOT = \"/data/atlas/users/lschoonh/BachelorProject/data/\"\n",
    "DATA_SAMPLE = DATA_ROOT + \"train_100_events/\"\n",
    "MODELS_ROOT = DIRECTORY + \"trained_models/2nd_place/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from same point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name: str = \"event000001001\"\n",
    "hits, cells, truth, particles = outrunner_code.get_event(event_name)\n",
    "preload = True\n",
    "\n",
    "my_preds: list[npt.NDArray] = find_file(f\"preds_{event_name}\", dir=DIRECTORY)  # type: ignore\n",
    "outrunner_preds = np.load(SOLUTION_DIR + \"my_%s.npy\" % event_name, allow_pickle=True)\n",
    "module_id = my_code.get_module_id(hits)\n",
    "PATH_THR = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run get_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_index = 0\n",
    "\n",
    "mask = np.ones(len(hits))\n",
    "\n",
    "# My path uses id, not index\n",
    "my_path_0 = my_code.get_path(hit_index + 1, thr=PATH_THR, mask=mask, module_id=module_id,preds=outrunner_preds) -1 \n",
    "outrunner_path_0 = outrunner_code.get_path2(hit_index, thr=PATH_THR, mask=mask, module_id=module_id, preds=outrunner_preds)\n",
    "\n",
    "print(\"my path\", my_path_0[:10])\n",
    "print(\"outrunner path\", outrunner_path_0[:10])\n",
    "print(\"Instances not in agreement: \", np.where(my_path_0 != outrunner_path_0)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with my preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_index = 0\n",
    "\n",
    "mask = np.ones(len(hits))\n",
    "\n",
    "# My path uses id, not index\n",
    "my_path_0 = my_code.get_path(hit_index + 1, thr=PATH_THR, mask=mask, module_id=module_id,preds=my_preds) -1 \n",
    "outrunner_path_0 = outrunner_code.get_path2(hit_index, thr=PATH_THR, mask=mask, module_id=module_id, preds=outrunner_preds)\n",
    "\n",
    "print(\"my path\", my_path_0[:10])\n",
    "print(\"outrunner path\", outrunner_path_0[:10])\n",
    "print(\"Instances not in agreement: \", np.where(my_path_0 != outrunner_path_0)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get n paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_limit = 1000 # generate only n paths\n",
    "my_new_tracks_limited = my_code.get_all_paths(hits, thr= PATH_THR, preds=outrunner_preds, module_id=module_id, debug_limit=debug_limit)\n",
    "\n",
    "outrunner_tracks_regenerated_limited: list[npt.NDArray] = outrunner_code.get_all_paths(hits, thr=PATH_THR, preds=outrunner_preds, module_id=module_id, debug_limit=debug_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first n' tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = debug_limit\n",
    "print(f\"{limit} my tracks\", my_new_tracks_limited[:limit])\n",
    "print(f\"{limit} outrunner tracks\", outrunner_tracks_regenerated_limited[:limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_track, verification_tracks in zip(my_new_tracks_limited[debug_limit], outrunner_tracks_regenerated_limited[debug_limit]):\n",
    "    assert np.all(test_track-1 == verification_tracks), \"Tracks are not equal\"\n",
    "print(f\"All first {debug_limit} tracks are equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare first track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtrackt 1 from all ids to get indices\n",
    "my_track_0 = my_new_tracks_limited[0]-1\n",
    "outrunner_track_0 = outrunner_tracks_regenerated_limited[0]\n",
    "\n",
    "print(\"my 1st tracks\", my_track_0)\n",
    "print(\"outrunner 1st tracks\", outrunner_track_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('isin' ,np.isin(my_track_0, outrunner_track_0, ))\n",
    "overlap = np.where(np.isin(my_track_0, outrunner_track_0) == True)\n",
    "outliers_1= np.where(np.isin(my_track_0, outrunner_track_0) == False)\n",
    "outliers_2= np.where(np.isin(outrunner_track_0, my_track_0) == False)\n",
    "print(\"overlap\", overlap)\n",
    "print(\"my hits not in outrunner\", outliers_1)\n",
    "print(\"outrunner hits not in mine\", outliers_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_model = my_code.load_model(SOLUTION_DIR + \"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, truth, particles = outrunner_code.get_event(event_name)\n",
    "hit_cells = cells.groupby(['hit_id']).value.count().values\n",
    "hit_value = cells.groupby(['hit_id']).value.sum().values\n",
    "hit_value = cells.groupby(['hit_id']).value.sum().values\n",
    "outrunner_features = np.hstack((hits[['x','y','z']]/1000, hit_cells.reshape(len(hit_cells),1)/10,hit_value.reshape(len(hit_cells),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just get_predict for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_index = 0\n",
    "my_p_0 = my_code.retrieve_predict(hit_index+1, preds=outrunner_preds)\n",
    "out_p_0 = outrunner_code.retrieve_predict(hit_index, preds=outrunner_preds)\n",
    "print(\"my p\", my_p_0[:15])\n",
    "print(\"out p\", out_p_0[:15])\n",
    "print(\"Instances not in agreement: \", np.where(my_p_0 != out_p_0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_0 = outrunner_preds[0][0]\n",
    "idx_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_preds[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_preds[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_my = my_code.make_predict(model=outrunner_model,features=outrunner_features, hits=hits, hit_id=hit_id, thr=0.85)\n",
    "p_my[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try getting preds for hit_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "i = hit_id -1\n",
    "model = outrunner_model\n",
    "\n",
    "# ------------------ 1st part ------------------\n",
    "\n",
    "TestX = np.zeros((len(outrunner_features), 10))\n",
    "TestX[:,5:] = outrunner_features\n",
    "\n",
    "# for TTA\n",
    "TestX1 = np.zeros((len(outrunner_features), 10))\n",
    "TestX1[:,:5] = outrunner_features\n",
    "\n",
    "preds = []\n",
    "\n",
    "TestX[i+1:,:5] = np.tile(outrunner_features[i], (len(TestX)-i-1, 1))\n",
    "pred = model.predict(TestX[i+1:], batch_size=batch_size, verbose=\"0\")[:,0]                \n",
    "idx = np.where(pred>0.2)[0]\n",
    "\n",
    "\n",
    "if len(idx) > 0:\n",
    "    TestX1[idx+i+1,5:] = TestX[idx+i+1,:5]\n",
    "    pred1 = model.predict(TestX1[idx+i+1], batch_size=batch_size, verbose=\"0\")[:,0]\n",
    "    pred[idx] = (pred[idx]+pred1)/2\n",
    "\n",
    "idx = np.where(pred>0.5)[0]\n",
    "\n",
    "p = np.zeros(len(pred), dtype=np.int32)\n",
    "# [idx+i+1, pred[idx]]\n",
    "# p[idx+i+1] = pred[idx]\n",
    "indices = idx+i+1\n",
    "p[indices] = pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_outrunner = my_code.retrieve_predict(hit_id, outrunner_preds)\n",
    "p_outrunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_outrunner_re = outrunner_code.get_predict(hit_index=hit_id-1, hits=hits,model=outrunner_model, features=outrunner_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get preds by preds_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr=0.85\n",
    "my_code.get_path(hit_id, thr=thr,mask=np.ones(len(hits)), module_id=module_id, preds=outrunner_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get preds by predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_code.get_path(hit_id, thr=thr,mask=np.ones(len(hits)), module_id=module_id, features=outrunner_features, model=outrunner_model, hits=hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "used_tracks = [outrunner_tracks, outrunner_tracks_regenerated][1]\n",
    "limit = None\n",
    "\n",
    "_make_my_scores = lambda: save(\n",
    "    my_get_track_scores(used_tracks, 8, limit, index_shift=0),\n",
    "    name=\"my_scores\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT,\n",
    ")\n",
    "_make_outrunner_scores = lambda: save(\n",
    "    outrunner_get_track_scores(used_tracks, 8, limit),\n",
    "    name=\"outrunner_scores\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT,\n",
    ")\n",
    "\n",
    "my_scores: npt.NDArray = find_file(\"my_scores\", dir=DIRECTORY, fallback_func=_make_my_scores, force_fallback=not preload)  # type: ignore\n",
    "\n",
    "outrunner_scores: npt.NDArray = find_file(\"outrunner_scores\", dir=DIRECTORY, fallback_func=_make_outrunner_scores, force_fallback=not preload)  # type: ignore\n",
    "\n",
    "limit = 5 if limit is None else limit\n",
    "print(\"My scores: \", my_scores[:limit])\n",
    "print(\"Outrunner scores: \", outrunner_scores[:limit])\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug grand scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_matrices(test_matrix: npt.NDArray | list[npt.NDArray], verification_matrix: npt.NDArray | list[npt.NDArray], limit: int | None = None):\n",
    "    for i, (test_row, verification_row) in tqdm(enumerate(zip(test_matrix, verification_matrix)), total=min(len(test_matrix), len(verification_matrix))):\n",
    "        if limit is not None and i >= limit:\n",
    "            break\n",
    "\n",
    "        tracks_equal = np.all(test_row == np.array(verification_row))\n",
    "        if not tracks_equal:\n",
    "            print(\"Rows are not equal\")\n",
    "            print(\"test row\", test_row)\n",
    "            print(\"good row\", verification_row)\n",
    "            print(\"Instances not in agreement: \", np.where(test_row!= verification_row)[0])\n",
    "            raise ValueError(\"Rows are not equal\")\n",
    "    print(f\"All first {i+1} rows are equal\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from same point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, cells, truth, particles = outrunner_code.get_event(event_name)\n",
    "hit_cells = cells.groupby(['hit_id']).value.count().values\n",
    "hit_value = cells.groupby(['hit_id']).value.sum().values\n",
    "hit_value = cells.groupby(['hit_id']).value.sum().values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_features = np.hstack((hits[['x','y','z']]/1000, hit_cells.reshape(len(hit_cells),1)/10,hit_value.reshape(len(hit_cells),1)))\n",
    "outrunner_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_features = my_code.get_featured_event(event_name).features\n",
    "my_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_matrices(my_features, outrunner_features, limit=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(my_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(outrunner_preds[0][0],outrunner_preds[0][1])\n",
    "# verify_matrices(my_preds, outrunner_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_model = my_code.load_model(SOLUTION_DIR + \"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = outrunner_features\n",
    "used_model = outrunner_model\n",
    "pred_matrix_limit = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred_matrix = my_code.make_predict_matrix(model=used_model,features=used_features, debug_limit=pred_matrix_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestX = np.zeros((len(used_features), 10))\n",
    "TestX[:,5:] = used_features\n",
    "\n",
    "# for TTA\n",
    "TestX1 = np.zeros((len(used_features), 10))\n",
    "TestX1[:,:5] = used_features\n",
    "\n",
    "preds = []\n",
    "\n",
    "for i in tqdm(range(pred_matrix_limit)):\n",
    "    TestX[i+1:,:5] = np.tile(used_features[i], (len(TestX)-i-1, 1))\n",
    "\n",
    "    pred = used_model.predict(TestX[i+1:], batch_size=20000,verbose=\"0\")[:,0]                \n",
    "    idx = np.where(pred>0.2)[0]\n",
    "\n",
    "    if len(idx) > 0:\n",
    "        TestX1[idx+i+1,5:] = TestX[idx+i+1,:5]\n",
    "        pred1 = used_model.predict(TestX1[idx+i+1], batch_size=20000,verbose=\"0\")[:,0]\n",
    "        pred[idx] = (pred[idx]+pred1)/2\n",
    "\n",
    "    idx = np.where(pred>0.5)[0]\n",
    "\n",
    "    preds.append([idx+i+1, pred[idx]])\n",
    "\n",
    "    #if i==0: print(preds[-1])\n",
    "\n",
    "preds.append([np.array([], dtype='int64'), np.array([], dtype='float32')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_matrices(outrunner_preds[0], preds[0], limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rebuild to NxN\n",
    "for i in range(len(preds)):\n",
    "    ii = len(preds)-i-1\n",
    "    for j in range(len(preds[ii][0])):\n",
    "        jj = preds[ii][0][j]\n",
    "        preds[jj][0] = np.insert(preds[jj][0], 0 ,ii)\n",
    "        preds[jj][1] = np.insert(preds[jj][1], 0 ,preds[ii][1][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_pred_matrix = outrunner_code.make_predict_matrix(model=used_model,features=used_features, debug_limit=pred_matrix_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions that produce and then save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict paths\n",
    "_make_tracks = lambda: save(\n",
    "    my_code.get_all_paths(hits, PATH_THR, module_id=module_id, preds=outrunner_preds, do_redraw=True),\n",
    "    name=\"new_tracks_all\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT\n",
    ")\n",
    "\n",
    "\n",
    "# calculate track's confidence\n",
    "_make_scores = lambda: save(\n",
    "    my_code.get_track_scores(tracks_all), name=\"new_scores\", tag=event_name, prefix=DIRECTORY, save=DO_EXPORT\n",
    ")\n",
    "\n",
    "# Merge seeds to definite id's\n",
    "_make_merged_tracks = lambda: save(\n",
    "    my_code.run_merging(tracks_all, scores, preds=outrunner_preds, multi_stage=True, module_id=module_id,log_evaluations=True, truth=truth),\n",
    "    name=\"merged_tracks\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT,\n",
    ")  # type: ignore\n",
    "\n",
    "# Save submission\n",
    "_make_submission = lambda: save(\n",
    "    pd.DataFrame({\"hit_id\": hits.hit_id, \"track_id\": merged_tracks}),\n",
    "    name=\"submission\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=do_export,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind result is not loaded if `preload` is False\n",
    "print(f\"Preload: {preload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_all: list[npt.NDArray] = find_file(f\"new_tracks_all_{event_name}\", dir=DIRECTORY, fallback_func=_make_tracks, force_fallback=not preload)  # type: ignore\n",
    "outrunner_tracks: list[npt.NDArray] = find_file(f\"outrunner_tracks_all\", dir=DIRECTORY, extension='pkl')  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tracks = tracks_all\n",
    "verification_tracks = outrunner_tracks # outrunner_tracks_regenerated_limited\n",
    "\n",
    "for i, (test_track, verification_tracks) in tqdm(enumerate(zip(test_tracks, verification_tracks)), total=min(len(test_tracks), len(verification_tracks))):\n",
    "    verification_tracks = np.array(verification_tracks)\n",
    "    test_track = np.array(test_track) - 1\n",
    "    tracks_equal = np.all(test_track == np.array(verification_tracks))\n",
    "    if not tracks_equal:\n",
    "        print(\"Tracks are not equal\")\n",
    "        print(\"test track\", test_track)\n",
    "        print(\"good track\", verification_tracks)\n",
    "        print(\"Instances not in agreement: \", np.where(test_track!= verification_tracks)[0])\n",
    "        raise ValueError(\"Tracks are not equal\")\n",
    "print(f\"All first {i+1} tracks are equal\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scores: npt.NDArray = find_file(f\"new_scores_{event_name}\", dir=DIRECTORY, fallback_func=_make_scores, force_fallback=not preload)  # type: ignore\n",
    "outrunner_scores: npt.NDArray = find_file(f\"outrunner_scores_{event_name}\", dir=DIRECTORY)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare a few scores by eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=10\n",
    "print(my_scores[:limit])\n",
    "print(outrunner_scores[:limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Equal: \", verify_matrices(my_scores, outrunner_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tracks: npt.NDArray = find_file(f\"merged_tracks_{event_name}\", dir=DIRECTORY, fallback_func=_make_merged_tracks, force_fallback=not preload)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make submission `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = find_file(\n",
    "    f\"submission_{event_name}\", dir=DIRECTORY, fallback_func=_make_submission, force_fallback=not preload\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = my_code.score_event(truth, submission)\n",
    "print(\"TrackML Score:\", score)\n",
    "print(\"Fast score: \", my_code.score_event_fast(submission, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add our track_id to truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined: pd.DataFrame = truth[[\"hit_id\", \"particle_id\", \"weight\", \"tx\", \"ty\", \"tz\"]].merge(submission, how=\"left\", on=\"hit_id\") # type: ignore\n",
    "# Group by unique combinations of track_id (our) and particle_id (truth); count number of hits overlapping\n",
    "grouped: pd.DataFrame = (\n",
    "    combined.groupby([\"track_id\", \"particle_id\"]).hit_id.count().to_frame(\"count_both\").reset_index()\n",
    ")\n",
    "# Skip unallocated tracks (track_id == 0)\n",
    "print(grouped[grouped['track_id'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select most likely related true `particle_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks are already ordered by score\n",
    "track_id = 2000\n",
    "possible_particle_ids: pd.DataFrame = grouped[grouped[\"track_id\"] == track_id].sort_values(\n",
    "    \"count_both\", ascending=False\n",
    ")\n",
    "most_likely_particle_id = int(possible_particle_ids.iloc[0][\"particle_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select related truth and reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_track = combined[combined[\"track_id\"] == track_id]\n",
    "truth_track = combined[combined[\"particle_id\"] == most_likely_particle_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected track ids: \\n\", reconstructed_track['hit_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstructed_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_weight_total = reconstructed_track[\"weight\"].sum()\n",
    "reconstructed_weight_overlap = reconstructed_track[reconstructed_track['particle_id'] == most_likely_particle_id]['weight'].sum()\n",
    "\n",
    "truth_weight = truth_track[\"weight\"].sum()\n",
    "\n",
    "ratio = reconstructed_weight_overlap / truth_weight\n",
    "\n",
    "print (f\"Track {track_id} has total weight {reconstructed_weight_total}, vs {truth_weight} from particle {most_likely_particle_id}, ratio: {ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig: plt.Figure = my_code.plot_prediction(truth_track, reconstructed_track, most_likely_particle_id, label_type=\"particle_id\")\n",
    "fig.suptitle(f\"Track {track_id} with particle id {most_likely_particle_id} \\n\\\n",
    "             weight ratio: {ratio:.2f}\\\n",
    "             \", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_save_fig = False\n",
    "if do_save_fig:\n",
    "    fig.savefig(f\"reconstructed_track_{track_id}_{event_name}.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link reconstructed tracks with true tracks and add primary vertex information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define minimal hits for truth and reconstructed tracks to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hits = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count reconstructed tracks hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_count = combined[combined['track_id']!=0].value_counts('track_id').to_frame('count_reco').sort_index()\n",
    "r_considered = r_count[r_count['count_reco'] >= min_hits]\n",
    "r_considered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter discarded tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tracks = grouped[(grouped['particle_id'] != 0) & (grouped['track_id'] != 0) & (grouped['count_both'] > 1)].reset_index(drop=True)\n",
    "valid_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count trutht tracks hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count = truth[truth['particle_id'] != 0].groupby('particle_id').count().hit_id.sort_values(ascending=False).to_frame('count_truth')\n",
    "p_considered = p_count[p_count['count_truth'] >= min_hits]\n",
    "p_considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm too many matches for amount of particles, should consider only primary particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of matches: \", len(valid_tracks))\n",
    "print(\"Number of particles\",  len(truth.particle_id.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine counts on considered truth and reco tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_pairs = valid_tracks[valid_tracks['particle_id'].isin(p_considered.index)][valid_tracks['track_id'].isin(r_considered.index)].sort_values(['count_both', 'track_id'], ascending=[False, True]).reset_index(drop=True).merge(p_considered, on='particle_id').merge(r_considered, on='track_id')\n",
    "considered_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find particles linked to too many reco tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_mask = considered_pairs[considered_pairs.duplicated(subset='particle_id', keep='first')].sort_index()\n",
    "duplicates_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select majority track for reco's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_matched =  considered_pairs[~considered_pairs.index.isin(duplicates_mask.index)]\n",
    "primary_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define 'good' tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_purity = primary_matched['count_both'] / primary_matched['count_reco']\n",
    "track_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_purity = primary_matched['count_both'] / primary_matched['count_truth']\n",
    "particle_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_matched.insert(4, 'particle_purity', particle_purity)\n",
    "primary_matched.insert(6, 'track_purity', track_purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Both ratios have to be above 50% to define a good track so that a one-to-one relationship between particle and track can be defined._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tracks = primary_matched[primary_matched['particle_purity'] >= 0.5]\n",
    "good_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency = len(good_tracks) / len(p_considered)\n",
    "print(f\"Efficiency: {100*efficiency:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find primary vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_all = np.sqrt(np.sum(truth[['tx', 'ty', 'tz']].values**2, axis=1))\n",
    "truth.insert(2, 'r', r_all)\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sorted = truth.sort_values('r', ascending=True)\n",
    "r_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mask = r_sorted[r_sorted.duplicated(subset='particle_id', keep='first')]\n",
    "r_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0 = r_sorted[~r_sorted.index.isin(r_mask.index)][['particle_id','r']].rename(columns={'r': 'r_0'})\n",
    "r_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define primary vertex hit values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_vertex_index = r_0.index\n",
    "primary_vertex_data = truth.loc[primary_vertex_index]\n",
    "primary_vertex_data.rename(columns={'r':'r_0','tx': 'x_0', 'ty': 'y_0', 'tz': 'z_0', 'tpx': 'px_0', 'tpy':'py_0', 'tpz': 'pz_0', 'weight':'weight_0'}, inplace=True)\n",
    "primary_vertex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_vertex_data['p_0']=np.sqrt(primary_vertex_data['px_0']**2+primary_vertex_data['py_0']**2+primary_vertex_data['pz_0']**2)\n",
    "primary_vertex_data['p_t_0']=np.sqrt(primary_vertex_data['px_0']**2+primary_vertex_data['py_0']**2)\n",
    "primary_vertex_data['log_10_p_t_0']=np.log10(primary_vertex_data['p_t_0'])\n",
    "primary_vertex_data['phi_0'] = np.arctan2(primary_vertex_data['y_0'], primary_vertex_data['x_0'])\n",
    "primary_vertex_data['theta_0'] = np.arccos(primary_vertex_data['z_0']/primary_vertex_data['r_0'])\n",
    "primary_vertex_data['pseudo_rapidity_0'] = -np.log(np.tan(primary_vertex_data['theta_0']/2))\n",
    "primary_vertex_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tracks = good_tracks.merge(primary_vertex_data, on='particle_id')\n",
    "good_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_considered = p_considered.merge(primary_vertex_data, on='particle_id')\n",
    "p_considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histograms of primary vertex information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_histograms(truth, test, variable: str | None = None,bins=100, range=None, density=False, title=None, xlabel=None, ylabel=None, figsize=(10, 6), **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.hist([truth[variable], test[variable]], bins=bins, range=range, density=density, alpha=0.75,**kwargs)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['r_0', 'p_0', 'p_t_0', 'log_10_p_t_0', 'phi_0', 'theta_0', 'pseudo_rapidity_0']\n",
    "var_labels = ['vertex $r_0$ [mm]', '$p$', '$P_{T}$', '$log_{10}$ $p_{T}$', '$\\\\phi$', '$\\\\theta$', '$\\\\eta$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable, label in zip( variables, var_labels):\n",
    "    fig = compare_histograms(p_considered, good_tracks, variable=variable, bins=100,title=f'Primary vertex {label}', xlabel=label, ylabel='Frequence', label=['Truth', 'Reconstructed'], histtype='step', linewidth=1,color=['blue', 'orange'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate efficiency over primary vertex variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficiency(truth: pd.DataFrame, test: pd.DataFrame, variable: str | None = None, bins: int=100, min=None, max=None, title=None, xlabel='x', ylabel='Efficiency', figsize=(10, 6), **kwargs):\n",
    "    min_bin = truth[variable].min() if min is None else min\n",
    "    max_bin = truth[variable].max() if max is None else max\n",
    "    bins_index = pd.cut(pd.Series([min_bin, max_bin]), bins=bins, retbins=True)[1]\n",
    "    print(label, min_bin, max_bin)\n",
    "    \n",
    "    cut_truth = pd.cut(truth[variable], bins=bins_index)  # type: ignore\n",
    "    groups_truth = truth.groupby(cut_truth)\n",
    "    binned_truth = groups_truth.count()[variable]\n",
    "    \n",
    "    cut_test = pd.cut(test[variable], bins=bins_index)  # type: ignore\n",
    "    groups_test = test.groupby(cut_test)\n",
    "    binned_test = groups_test.count()[variable]\n",
    "    \n",
    "    efficiency =  binned_test/binned_truth\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    x = np.arange(min_bin, max_bin, (max_bin-min_bin)/bins)\n",
    "    yerr = np.sqrt(binned_test)/binned_truth\n",
    "    ax.plot(x, efficiency.values, label=ylabel, **kwargs)\n",
    "    ax.fill_between(x, efficiency-yerr, efficiency + yerr, alpha=0.5, label='Error')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_title(f'Efficiency by {xlabel}' if title is None else title)\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['r_0', 'r_0', 'p_0', 'p_t_0', 'log_10_p_t_0', 'phi_0', 'theta_0', 'pseudo_rapidity_0']\n",
    "var_labels = ['vertex $r_0$ [mm]','zoom vertex $r_0$ [mm]', '$p$', '$P_{T}$', '$log_{10}$ $p_{T}$', '$\\\\phi$', '$\\\\theta$', '$\\\\eta$']\n",
    "mins = [0, 0,None, 0, None, -np.pi, -np.pi, -np.pi]\n",
    "maxs = [600,100,25, 5, 2,np.pi, np.pi, np.pi]\n",
    "for variable, label, min, max in zip( variables, var_labels, mins, maxs):\n",
    "    fig = efficiency(p_considered, good_tracks, variable=variable, bins=100,min=min, max=max, xlabel=label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrootenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
