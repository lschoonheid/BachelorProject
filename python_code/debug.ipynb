{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get things ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from data_exploration.helpers import find_file, save\n",
    "from tqdm import tqdm\n",
    "import trackml_copy as outrunner_code\n",
    "import trackml_2_solution_example as my_code\n",
    "\n",
    "DO_EXPORT = True\n",
    "DIRECTORY = my_code.DIRECTORY\n",
    "SOLUTION_DIR = my_code.SOLUTION_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from same point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_name: str = \"event000001001\"\n",
    "hits, cells, truth, particles = outrunner_code.get_event(event_name)\n",
    "preload = True\n",
    "\n",
    "outrunner_preds = np.load(SOLUTION_DIR + \"my_%s.npy\" % event_name, allow_pickle=True)\n",
    "module_id = my_code.get_module_id(hits)\n",
    "PATH_THR = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just run get_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_index = 0\n",
    "\n",
    "mask = np.ones(len(hits))\n",
    "\n",
    "# My path uses id, not index\n",
    "my_path_0 = my_code.get_path(hit_index + 1, thr=PATH_THR, mask=mask, module_id=module_id,preds=outrunner_preds) -1 \n",
    "outrunner_path_0 = outrunner_code.get_path2(hit_index, thr=PATH_THR, mask=mask, module_id=module_id, preds=outrunner_preds)\n",
    "\n",
    "print(\"my path\", my_path_0[:10])\n",
    "print(\"outrunner path\", outrunner_path_0[:10])\n",
    "print(\"Instances not in agreement: \", np.where(my_path_0 != outrunner_path_0)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get n paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_limit = 1000 # generate only n paths\n",
    "my_new_tracks_limited = my_code.get_all_paths(hits, thr= PATH_THR, preds=outrunner_preds, module_id=module_id, debug_limit=debug_limit)\n",
    "\n",
    "outrunner_tracks_regenerated_limited: list[npt.NDArray] = outrunner_code.get_all_paths(hits, thr=PATH_THR, preds=outrunner_preds, module_id=module_id, debug_limit=debug_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first n' tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = debug_limit\n",
    "print(f\"{limit} my tracks\", my_new_tracks_limited[:limit])\n",
    "print(f\"{limit} outrunner tracks\", outrunner_tracks_regenerated_limited[:limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_track, verification_tracks in zip(my_new_tracks_limited[debug_limit], outrunner_tracks_regenerated_limited[debug_limit]):\n",
    "    assert np.all(test_track-1 == verification_tracks), \"Tracks are not equal\"\n",
    "print(f\"All first {debug_limit} tracks are equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare first track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtrackt 1 from all ids to get indices\n",
    "my_track_0 = my_new_tracks_limited[0]-1\n",
    "outrunner_track_0 = outrunner_tracks_regenerated_limited[0]\n",
    "\n",
    "print(\"my 1st tracks\", my_track_0)\n",
    "print(\"outrunner 1st tracks\", outrunner_track_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('isin' ,np.isin(my_track_0, outrunner_track_0, ))\n",
    "overlap = np.where(np.isin(my_track_0, outrunner_track_0) == True)\n",
    "outliers_1= np.where(np.isin(my_track_0, outrunner_track_0) == False)\n",
    "outliers_2= np.where(np.isin(outrunner_track_0, my_track_0) == False)\n",
    "print(\"overlap\", overlap)\n",
    "print(\"my hits not in outrunner\", outliers_1)\n",
    "print(\"outrunner hits not in mine\", outliers_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just get_predict for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_index = 0\n",
    "my_p_0 = my_code.retrieve_predict(hit_index+1, preds=outrunner_preds)\n",
    "out_p_0 = outrunner_code.retrieve_predict(hit_index, preds=outrunner_preds)\n",
    "print(\"my p\", my_p_0[:15])\n",
    "print(\"out p\", out_p_0[:15])\n",
    "print(\"Instances not in agreement: \", np.where(my_p_0 != out_p_0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "used_tracks = [outrunner_tracks, outrunner_tracks_regenerated][1]\n",
    "limit = None\n",
    "\n",
    "_make_my_scores = lambda: save(\n",
    "    my_get_track_scores(used_tracks, 8, limit, index_shift=0),\n",
    "    name=\"my_scores\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT,\n",
    ")\n",
    "_make_outrunner_scores = lambda: save(\n",
    "    outrunner_get_track_scores(used_tracks, 8, limit),\n",
    "    name=\"outrunner_scores\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT,\n",
    ")\n",
    "\n",
    "my_scores: npt.NDArray = find_file(\"my_scores\", dir=DIRECTORY, fallback_func=_make_my_scores, force_fallback=not preload)  # type: ignore\n",
    "\n",
    "outrunner_scores: npt.NDArray = find_file(\"outrunner_scores\", dir=DIRECTORY, fallback_func=_make_outrunner_scores, force_fallback=not preload)  # type: ignore\n",
    "\n",
    "limit = 5 if limit is None else limit\n",
    "print(\"My scores: \", my_scores[:limit])\n",
    "print(\"Outrunner scores: \", outrunner_scores[:limit])\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug grand scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_matrices(test_matrix: npt.NDArray | list[npt.NDArray], verification_matrix: npt.NDArray | list[npt.NDArray], limit: int | None = None):\n",
    "    for i, (test_row, verification_row) in tqdm(enumerate(zip(test_matrix, verification_matrix)), total=min(len(test_matrix), len(verification_matrix))):\n",
    "        if limit is not None and i >= limit:\n",
    "            break\n",
    "\n",
    "        tracks_equal = np.all(test_row == np.array(verification_row))\n",
    "        if not tracks_equal:\n",
    "            print(\"Rows are not equal\")\n",
    "            print(\"test row\", test_row)\n",
    "            print(\"good row\", verification_row)\n",
    "            print(\"Instances not in agreement: \", np.where(test_row!= verification_row)[0])\n",
    "            raise ValueError(\"Rows are not equal\")\n",
    "    print(f\"All first {i+1} rows are equal\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some functions that produce and then save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict paths\n",
    "_make_tracks = lambda: save(\n",
    "    my_code.get_all_paths(hits, PATH_THR, module_id=module_id, preds=outrunner_preds, do_redraw=True),\n",
    "    name=\"new_tracks_all\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT\n",
    ")\n",
    "\n",
    "\n",
    "# calculate track's confidence\n",
    "_make_scores = lambda: save(\n",
    "    my_code.get_track_scores(tracks_all), name=\"new_scores\", tag=event_name, prefix=DIRECTORY, save=DO_EXPORT\n",
    ")\n",
    "\n",
    "# Merge seeds to definite id's\n",
    "_make_merged_tracks = lambda: save(\n",
    "    my_code.run_merging(tracks_all, scores, preds=outrunner_preds, multi_stage=True, module_id=module_id,log_evaluations=True, truth=truth),\n",
    "    name=\"merged_tracks\",\n",
    "    tag=event_name,\n",
    "    prefix=DIRECTORY,\n",
    "    save=DO_EXPORT,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind result is not loaded if `preload` is False\n",
    "print(f\"Preload: {preload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_all: list[npt.NDArray] = find_file(f\"new_tracks_all_{event_name}\", dir=DIRECTORY, fallback_func=_make_tracks, force_fallback=not preload)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outrunner_tracks: list[npt.NDArray] = find_file(f\"outrunner_tracks_all\", dir=DIRECTORY, extension='pkl')  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_tracks_limited = my_code.get_all_paths(hits, thr= PATH_THR, preds=outrunner_preds, module_id=module_id, debug_limit=debug_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tracks = tracks_all\n",
    "verification_tracks = outrunner_tracks # outrunner_tracks_regenerated_limited\n",
    "\n",
    "for i, (test_track, verification_tracks) in tqdm(enumerate(zip(test_tracks, verification_tracks)), total=min(len(test_tracks), len(verification_tracks))):\n",
    "    verification_tracks = np.array(verification_tracks)\n",
    "    test_track = np.array(test_track) - 1\n",
    "    tracks_equal = np.all(test_track == np.array(verification_tracks))\n",
    "    if not tracks_equal:\n",
    "        print(\"Tracks are not equal\")\n",
    "        print(\"test track\", test_track)\n",
    "        print(\"good track\", verification_tracks)\n",
    "        print(\"Instances not in agreement: \", np.where(test_track!= verification_tracks)[0])\n",
    "        raise ValueError(\"Tracks are not equal\")\n",
    "print(f\"All first {i+1} tracks are equal\") # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scores: npt.NDArray = find_file(f\"new_scores_{event_name}\", dir=DIRECTORY, fallback_func=_make_scores, force_fallback=not preload)  # type: ignore\n",
    "outrunner_scores: npt.NDArray = find_file(f\"outrunner_scores_{event_name}\", dir=DIRECTORY)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare a few scores by eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=10\n",
    "print(my_scores[:limit])\n",
    "print(outrunner_scores[:limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "are_same = verify_matrices(my_scores, outrunner_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tracks: npt.NDArray = find_file(f\"merged_tracks_{event_name}\", dir=DIRECTORY, fallback_func=_make_merged_tracks, force_fallback=not preload)  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myrootenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
